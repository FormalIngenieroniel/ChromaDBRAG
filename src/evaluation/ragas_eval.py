import os
import sys
import warnings
import time
import torch
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings

# --- 1. CONFIGURACI√ìN DE RUTAS E IMPORTS ---
# Ajusta esto seg√∫n d√≥nde est√© este archivo. Si est√° en la ra√≠z, quita los '..'
# Si est√° en una carpeta 'tests', deja esto as√≠:
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

import config

# IMPORTS REALES DE TU PROYECTO
from src.components.retriever import search_chroma
from src.components.generator import generate_response

import pandas as pd
from datasets import Dataset

# Imports de RAGAS
try:
    from ragas.metrics import (
        Faithfulness,
        AnswerSimilarity,
        ContextPrecision,
        ContextRecall,
        ResponseRelevancy
    )
    from ragas import evaluate
    from ragas.run_config import RunConfig
except ImportError as e:
    print(f"Error importando Ragas: {e}")
    sys.exit(1)

# Configuraci√≥n de entorno
os.environ["OPENAI_API_KEY"] = "sk-no-key-needed"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
warnings.filterwarnings("ignore")

# --- 2. CONFIGURACI√ìN LLM JUEZ (RAGAS) ---
GOOGLE_API_KEY = config.GEMINI_API_KEY 

if "AIza" not in GOOGLE_API_KEY:
    print("‚ö†Ô∏è ¬°ALERTA! No has puesto tu API Key de Google.")
    sys.exit(1)

print(f"\nüîÑ Conectando con Google Gemini para Evaluaci√≥n...")

# Clase para evitar Rate Limit (Plan Gratuito)
class SlowGemini(ChatGoogleGenerativeAI):
    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
        time.sleep(5) # Espera 5 segundos entre evaluaciones
        return super()._generate(messages, stop=stop, run_manager=run_manager, **kwargs)

try:
    # Este LLM es SOLO para que RAGAS juzgue las respuestas
    ragas_llm = SlowGemini(
        model="gemini-2.5-flash-lite", # Recomiendo 1.5 Flash por estabilidad
        google_api_key=GOOGLE_API_KEY,
        temperature=0
    )
    print("‚úÖ Juez LLM Conectado.")
except Exception as e:
    print(f"\n‚ùå Error conectando Juez: {e}")
    sys.exit(1)

# --- 3. CONFIGURACI√ìN DE EMBEDDINGS (Para Ragas) ---
# Nota: Ragas necesita sus propios embeddings para calcular similitudes.
# Usamos el mismo modelo ligero que ten√≠as para no sobrecargar.
print("üîÑ Cargando Embeddings de Evaluaci√≥n...")
hf_embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# --- 4. DATASET DE PRUEBA (Ground Truth) ---
# Aqu√≠ defines las preguntas y cu√°l DEBER√çA ser la respuesta ideal.
# Esto es lo √∫nico manual que queda, ya que necesitas una verdad absoluta para comparar.
test_data = [
    {
        "question": "Necesito el vag√≥n cisterna que transporta petr√≥leo (NEFT).",
        "ground_truth": "El vag√≥n adecuado es el que aparece en la imagen 12.jpg. Es un vag√≥n cisterna de color rojo oscuro dise√±ado espec√≠ficamente para el transporte de petr√≥leo o materiales inflamables."
    },
    {
        "question": "Mu√©strame el vag√≥n de carga sellado de color azul marino profundo.",
        "ground_truth": "El vag√≥n correspondiente es el de la imagen 08.jpg. Se trata de un vag√≥n de carga tipo caja cerrada (boxcar) de color azul marino profundo."
    }
    # Puedes agregar m√°s preguntas aqu√≠ si tienes las respuestas correctas en tus descripciones
]

def run_evaluation():
    print("\n--- üìä Iniciando Evaluaci√≥n RAGAS con DATOS REALES ---")

    questions = []
    answers = []
    contexts = []
    ground_truths = []

    # --- BUCLE DE GENERACI√ìN REAL ---
    for item in test_data:
        q = item["question"]
        gt = item["ground_truth"]
        
        print(f"\nProcesando: '{q}'")
        
        # 1. RETRIEVER REAL
        # Busca en tu ChromaDB real
        retrieved_items_dicts = search_chroma(q, n_results=3)
        
        # 2. GENERADOR REAL
        # Usa tu generador (que llama a Gemini internamente)
        # Nota: Esto consumir√° cuota de tu API Key tambi√©n.
        generated_answer = generate_response(q, retrieved_items_dicts)
        
        # 3. PREPARAR CONTEXTO PARA RAGAS
        # Ragas espera una lista de strings ['info A', 'info B']
        # Tu retriever devuelve diccionarios, as√≠ que extraemos las descripciones.
        context_strings = [item.get('description', '') for item in retrieved_items_dicts]
        
        # Guardar en listas
        questions.append(q)
        answers.append(generated_answer)
        contexts.append(context_strings)
        ground_truths.append(gt)
        
        print("  ‚úÖ Respuesta generada.")
        # Pausa extra de seguridad para no saturar la API (Generaci√≥n + Evaluaci√≥n)
        time.sleep(2)

    # Crear Dataset de HuggingFace
    dataset = Dataset.from_dict({
        "question": questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths
    })

    # --- 5. EJECUCI√ìN DE M√âTRICAS ---
    metrics_to_run = [
        Faithfulness(),      # ¬øLa respuesta se basa en el contexto recuperado?
        AnswerSimilarity(),  # ¬øLa respuesta se parece a la Ground Truth?
        ContextPrecision(),  # ¬øEl contexto relevante apareci√≥ primero?
        ResponseRelevancy()  # ¬øLa respuesta tiene sentido con la pregunta?
    ]

    print("\nüöÄ Ejecutando m√©tricas de Ragas...")
    
    run_config = RunConfig(
        max_workers=1, # Un solo hilo para evitar rate limits
        timeout=600 
    )

    results = evaluate(
        dataset=dataset,
        metrics=metrics_to_run,
        llm=ragas_llm,       # El juez Gemini Lento
        embeddings=hf_embeddings,
        run_config=run_config
    )

    # --- 6. RESULTADOS ---
    print("\n================== üìà Resultados Detallados ==================")
    df_results = results.to_pandas()
    
    # Seleccionamos columnas para mostrar limpio
    cols_to_show = ['question', 'answer', 'faithfulness', 'answer_similarity', 'context_precision', 'response_relevancy']
    final_cols = [c for c in cols_to_show if c in df_results.columns]
    
    print(df_results[final_cols])
    
    print("\n--- Promedios Globales ---")
    print(results)
    
    df_results.to_csv("resultados_real_chroma.csv", index=False)
    print("\n‚úÖ Guardado en 'resultados_real_chroma.csv'")

if __name__ == "__main__":
    run_evaluation()